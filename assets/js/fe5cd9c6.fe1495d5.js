"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[3538],{5488:(e,t,n)=>{n.d(t,{A:()=>i});const i=n.p+"assets/images/api_arch-34c1a2451d59849a53fc3e04e0ddfec4.jpeg"},7889:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>s,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var i=n(4848),r=n(8453);const a={},o="API Introduction",l={id:"application-development/api/overview",title:"API Introduction",description:"GeaFlow API is a development interface provided for advanced users, which supports two types of APIs",source:"@site/../docs-en/source/5.application-development/1.api/1.overview.md",sourceDirName:"5.application-development/1.api",slug:"/application-development/api/overview",permalink:"/tugraph-analytics/en/application-development/api/overview",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:1,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"GeaFlow Console Platform",permalink:"/tugraph-analytics/en/concepts/console_principle"},next:{title:"Source Introduction",permalink:"/tugraph-analytics/en/application-development/api/stream/source"}},s={},c=[{value:"Maven Dependency",id:"maven-dependency",level:2},{value:"Overview Of Functions",id:"overview-of-functions",level:2},{value:"Graph API",id:"graph-api",level:3},{value:"Stream API",id:"stream-api",level:3},{value:"Typical Example",id:"typical-example",level:2},{value:"Introduction to PageRank Dynamic Graph Computing Example",id:"introduction-to-pagerank-dynamic-graph-computing-example",level:3},{value:"The Definition Of PageRank",id:"the-definition-of-pagerank",level:4},{value:"Example Code",id:"example-code",level:4},{value:"Introduction to PageRank Static Graph Computing Example",id:"introduction-to-pagerank-static-graph-computing-example",level:3},{value:"Example Code",id:"example-code-1",level:4},{value:"Introduction to WordCount Batch Computation Example",id:"introduction-to-wordcount-batch-computation-example",level:3},{value:"Example Code",id:"example-code-2",level:4},{value:"Introduction to KeyAgg stream computation example",id:"introduction-to-keyagg-stream-computation-example",level:3},{value:"Example Code",id:"example-code-3",level:4}];function d(e){const t={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"api-introduction",children:"API Introduction"})}),"\n",(0,i.jsx)(t.p,{children:"GeaFlow API is a development interface provided for advanced users, which supports two types of APIs: Graph API and Stream API:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Graph API: Graph is a first-class citizen of the GeaFlow framework. Currently, the GeaFlow framework provides a set of graph computing programming interfaces based on GraphView, including graph construction, graph computation, and traversal. In GeaFlow, both static and dynamic graphs are supported.\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Static Graph API: Static graph computing API. Full graph computing or traversal can be performed based on this API."}),"\n",(0,i.jsxs)(t.li,{children:["Dynamic Graph API: Dynamic graph computing API. GraphView is the data abstraction of a dynamic graph in GeaFlow. Based on GraphView, dynamic graph computing or traversal can be performed. At the same time, support for Snapshot generation from GraphView is provided. The Snapshot can provide the same interface capability as the Static Graph API.\n",(0,i.jsx)(t.img,{alt:"api_arch",src:n(5488).A+"",width:"630",height:"331"})]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(t.li,{children:["The Stream API: GeaFlow provides a set of programming interfaces for general computing, including source construction, streaming batch computing, and sink output. In GeaFlow, both Batch and Stream are supported.\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Batch API: Batch computing API, which can perform batch computation based on this API."}),"\n",(0,i.jsx)(t.li,{children:"Stream API: Streaming computing API. StreamView is the data abstraction of a dynamic stream in GeaFlow. Based on StreamView, streaming computing can be performed."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"From the introduction of the two types of APIs, it can be seen that GeaFlow unifies the graph view and stream view semantics through View internally. At the same time, in order to support two sets of APIs for dynamic and static computing, GeaFlow abstracts the concept of Window internally. Starting from the Source API, a Window must be included to split data windows based on the Window."}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"For streaming or dynamic graph APIs, the Window can be split by size, and each window reads a certain size of data to achieve incremental computation."}),"\n",(0,i.jsx)(t.li,{children:"For batch or static graph APIs, the Window will use the AllWindow mode, and a window will read the full amount of data to achieve full computation."}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"maven-dependency",children:"Maven Dependency"}),"\n",(0,i.jsx)(t.p,{children:"To develop a GeaFlow API application, you need to add maven dependencies:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-xml",children:"<dependency>\n    <groupId>com.antgroup.tugraph</groupId>\n    <artifactId>geaflow-api</artifactId>\n    <version>0.1</version>\n</dependency>\n\n<dependency>\n    <groupId>com.antgroup.tugraph</groupId>\n    <artifactId>geaflow-pdata</artifactId>\n    <version>0.1</version>\n</dependency>\n\n<dependency>\n    <groupId>com.antgroup.tugraph</groupId>\n    <artifactId>geaflow-cluster</artifactId>\n    <version>0.1</version>\n</dependency>\n\n<dependency>\n    <groupId>com.antgroup.tugraph</groupId>\n    <artifactId>geaflow-on-local</artifactId>\n    <version>0.1</version>\n</dependency>\n\n<dependency>\n    <groupId>com.antgroup.tugraph</groupId>\n    <artifactId>geaflow-pipeline</artifactId>\n    <version>0.1</version>\n</dependency>\n"})}),"\n",(0,i.jsx)(t.h2,{id:"overview-of-functions",children:"Overview Of Functions"}),"\n",(0,i.jsx)(t.h3,{id:"graph-api",children:"Graph API"}),"\n",(0,i.jsx)(t.p,{children:"Graph API is a first-class citizen in GeaFlow, which provides a set of graph computing programming interfaces based on GraphView, including graph construction, graph computation, and traversal. The specific API descriptions are shown in the following table:"}),"\n",(0,i.jsxs)(t.table,{children:["\n\t",(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"Type"}),"\n\t\t",(0,i.jsx)(t.td,{children:"API"}),"\n\t\t",(0,i.jsx)(t.td,{children:"Explanation"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{rowSpan:"7",children:"Dynamic Graph"}),"\n\t\t",(0,i.jsx)(t.td,{children:"PGraphView init(GraphViewDesc graphViewDesc)"}),"\n\t\t",(0,i.jsx)(t.td,{children:"Initialize with graphViewDesc as input"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PGraphView PIncGraphView appendVertex(PWindowStream vertexStream)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Using a distributed vertex stream as the incremental set of graph vertices for GraphView"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PIncGraphView appendEdge(PWindowStream edgeStream)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Using a distributed edge stream as the incremental set of graph edges for GraphView"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PIncGraphView appendGraph(PWindowStream vertexStream, PWindowStream edgeStream)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Using a distributed vertex and edge stream as the incremental set of graph vertices/edges for GraphView"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:" PGraphTraversal incrementalTraversal(IncVertexCentricTraversal incVertexCentricTraversal)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing incremental graph traversal on a dynamic GraphView"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PGraphCompute incrementalCompute(IncVertexCentricCompute incVertexCentricCompute)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing incremental graph computation on a dynamic GraphView"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"void materialize()     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Storing the incremental set of points and edges from a dynamic GraphView into state "}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{rowSpan:"7",children:"Static Graph"}),"\n\t\t",(0,i.jsx)(t.td,{children:" PGraphCompute compute(VertexCentricCompute vertexCentricCompute)"}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing static graph vertex centric computation on Graph"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:" PGraphWindow compute(ScatterGatherCompute sgAlgorithm, int parallelism)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing static graph scatter gather computation on Graph"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:" PGraphTraversal traversal(VertexCentricTraversal vertexCentricTraversal)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing static graph vertex centric traversal on Graph"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:" PWindowStream getEdges()     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Return a collection of edges"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PWindowStream getVertices()     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Return a collection of vertices "}),"\n\t"]})]})]}),"\n",(0,i.jsx)(t.h3,{id:"stream-api",children:"Stream API"}),"\n",(0,i.jsx)(t.p,{children:"The Stream API provides a set of programming interfaces for general computation, including source construction, stream and batch computing, and sink output. The specific API documentation is shown in the table below:"}),"\n",(0,i.jsxs)(t.table,{children:["\n\t",(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"Type"}),"\n\t\t",(0,i.jsx)(t.td,{children:"API"}),"\n\t\t",(0,i.jsx)(t.td,{children:"Explanation"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{rowSpan:"4",children:"Stream"}),"\n\t\t",(0,i.jsx)(t.td,{children:"PStreamView init(IViewDesc viewDesc)"}),"\n\t\t",(0,i.jsx)(t.td,{children:"Initializing with a StreamViewDesc"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PIncStreamView append(PWindowStream windowStream)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Using distributed data as an incremental dataset for a StreamView"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PWindowStream reduce(ReduceFunction reduceFunction)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing incremental reduce aggregation on a dynamic StreamView"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:" PWindowStream aggregate(AggregateFunction aggregateFunction)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing incremental aggregate aggregation on a dynamic StreamView"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{rowSpan:"12",children:"Batch"}),"\n\t\t",(0,i.jsx)(t.td,{children:"PStreamView  PWindowStream map(MapFunction mapFunction)"}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing map operation"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PWindowStream filter(FilterFunction filterFunction)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing filter operation"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:" PWindowStream flatMap(FlatMapFunction flatMapFunction)      "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing flatMap operation"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:" PWindowStream union(PStream uStream)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing union merge on two streams"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PWindowBroadcastStream broadcast()     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Broadcasting a stream downstream"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:" PWindowKeyStream keyBy(KeySelector selectorFunction)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing keyby operation based on selectorFunction rules"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PStreamSink sink(SinkFunction sinkFunction)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Outputting the results"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PWindowCollect collect()     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Triggering the collection of data results"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:"PWindowStream reduce(ReduceFunction reduceFunction)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing a reduce aggregation calculation within a window"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:" PWindowStream aggregate(AggregateFunction aggregateFunction)     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Performing a aggregate aggregation calculation within a window"}),"\n\t"]}),"\n\t",(0,i.jsxs)(t.tr,{children:["\n\t\t",(0,i.jsx)(t.td,{children:" PIncStreamView materialize()     "}),"\n\t\t",(0,i.jsx)(t.td,{children:"Using a PWindowKeyStream as a dynamic StreamView and generating an IncStreamView after default keyby"}),"\n\t"]})]})]}),"\n",(0,i.jsx)(t.h2,{id:"typical-example",children:"Typical Example"}),"\n",(0,i.jsx)(t.h3,{id:"introduction-to-pagerank-dynamic-graph-computing-example",children:"Introduction to PageRank Dynamic Graph Computing Example"}),"\n",(0,i.jsx)(t.h4,{id:"the-definition-of-pagerank",children:"The Definition Of PageRank"}),"\n",(0,i.jsx)(t.p,{children:"PageRank algorithm was originally used to calculate the importance of Internet pages. It was proposed by Page and Brin in 1996 and used in the ranking of web pages in Google search engine. In fact, PageRank can be defined on any digraph and later applied to many problems such as social influence analysis, text summary and so on.\nAssuming that the Internet is a directed graph, the random walk model is defined on the basis of which, namely, the first-order Markov chain, represents the process of web page visitors browsing the web pages randomly on the Internet. It is assumed that the viewer jumps to the next page with equal probability according to the hyperlink connected out in each page, and continues to carry out such a random jump on the Internet, this process forms a first-order Markov chain. PageRank represents the smooth distribution of this Markov chain. The PageRank value of each page is the stationary probability.\nThe implementation of the algorithm is as follows: 1. Assume that the initial influence value of each point in the figure is the same; 2. 2. Calculate the jump probability of each point to other points, and update the influence value of the point; 3. Perform n iterations until the influence value of each point no longer changes, that is the convergence state."}),"\n",(0,i.jsx)(t.h4,{id:"example-code",children:"Example Code"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-java",children:'\npublic class IncrGraphCompute {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(IncrGraphCompute.class);\n\t// Computed result path.\n    public static final String RESULT_FILE_PATH = "./target/tmp/data/result/incr_graph";\n\t// Comparison result path.\n    public static final String REF_FILE_PATH = "data/reference/incr_graph";\n\n    public static void main(String[] args) {\n\t\t// Get execution environment.\n        Environment environment = EnvironmentFactory.onLocalEnvironment();\n\t\t// Perform job submission.\n        IPipelineResult result = submit(environment);\n\t\t// Wait for execution to complete.\n        result.get();\n\t\t// Close the execution environment.\n\t\tenvironment.shutdown();\n    }\n\n    public static IPipelineResult<?> submit(Environment environment) {\n\t\t// Build the task execution flow.\n        final Pipeline pipeline = PipelineFactory.buildPipeline(environment);\n\t\t// Get the job environment configuration.\n        Configuration envConfig = environment.getEnvironmentContext().getConfig();\n\t\t// Specifies the path to save the calculation results.\n        envConfig.put(FileSink.OUTPUT_DIR, RESULT_FILE_PATH);\n\n        // Graphview name.\n        final String graphName = "graph_view_name";\n\t\t// Create delta graph graphview.\n        GraphViewDesc graphViewDesc = GraphViewBuilder\n\t\t\t.createGraphView(graphName)\n\t\t\t// Set the number of graphview shards, which can be specified from the configuration.\n            .withShardNum(envConfig.getInteger(ExampleConfigKeys.ITERATOR_PARALLELISM))\n\t\t\t// Set graphview backend type.\n            .withBackend(BackendType.RocksDB)\n\t\t\t// Specify graphview schema information such as vertices/edges and attributes.\n            .withSchema(new GraphMetaType(IntegerType.INSTANCE, ValueVertex.class, Integer.class, ValueEdge.class, IntegerType.class))\n            .build();\n\t\t// Add the created graphview information to the task execution flow.\n        pipeline.withView(graphName, graphViewDesc);\n\t\t\n\t\t// Submit the task and execute.\n        pipeline.submit(new PipelineTask() {\n            @Override\n            public void execute(IPipelineTaskContext pipelineTaskCxt) {\n                Configuration conf = pipelineTaskCxt.getConfig();\n\t\t\t\t// 1. Build vertex data input source.\n                PWindowSource<IVertex<Integer, Integer>> vertices =\n                    // Extract vertex from edge file.\n                    pipelineTaskCxt.buildSource(new RecoverableFileSource<>("data/input/email_edge",\t\t\t\n\t\t\t\t\t\t// Specifies the parsing format for each row of data.\n                        line -> {\n                            String[] fields = line.split(",");\n                            IVertex<Integer, Integer> vertex1 = new ValueVertex<>(\n                                Integer.valueOf(fields[0]), 1);\n                            IVertex<Integer, Integer> vertex2 = new ValueVertex<>(\n                                Integer.valueOf(fields[1]), 1);\n                            return Arrays.asList(vertex1, vertex2);\n                        }), SizeTumblingWindow.of(10000))\n\t\t\t\t\t\t// Specifies the parallelism of vertex data sources.\n                        .withParallelism(pipelineTaskCxt.getConfig().getInteger(ExampleConfigKeys.SOURCE_PARALLELISM));\n\t\t\t\t\n\t\t\t\t// 2. Build the edge data input source.\n                PWindowSource<IEdge<Integer, Integer>> edges =\n                    pipelineTaskCxt.buildSource( new RecoverableFileSource<>("data/input/email_edge",\t\t\t\n\t\t\t\t\t\t// Specifies the parsing format for each row of data.\n                        line -> {\n                            String[] fields = line.split(",");\n                            IEdge<Integer, Integer> edge = new ValueEdge<>(Integer.valueOf(fields[0]),\n                                Integer.valueOf(fields[1]), 1);\n                            return Collections.singletonList(edge);\n                        }), SizeTumblingWindow.of(5000))\n\t\t\t\t\t\t// Specifies the parallelism of edge data sources.\n\t\t\t\t\t\t.withParallelism(pipelineTaskCxt.getConfig().getInteger(ExampleConfigKeys.SOURCE_PARALLELISM));\n\t\t\t\t\n\t\t\t\t// Get the defined graphview and build the graph data.\n                PGraphView<Integer, Integer, Integer> fundGraphView =\n                    pipelineTaskCxt.getGraphView(graphName);\n                PIncGraphView<Integer, Integer, Integer> incGraphView =\n                    fundGraphView.appendGraph(vertices, edges);\n\t\t\t\t// Get the concurrency of Map tasks running in a job.\n                int mapParallelism = pipelineTaskCxt.getConfig().getInteger(ExampleConfigKeys.MAP_PARALLELISM);\n\t\t\t\t// Get the concurrency of Sink tasks running in a job.\n                int sinkParallelism = pipelineTaskCxt.getConfig().getInteger(ExampleConfigKeys.SINK_PARALLELISM);\n\t\t\t\t// Create sink method.\n                SinkFunction<String> sink = ExampleSinkFunctionFactory.getSinkFunction(conf);\t\t\t\n\t\t\t\t// Based on graph algorithm, dynamic graph calculation is performed.\n                incGraphView.incrementalCompute(new IncGraphAlgorithms(3))\n\t\t\t\t\t// Get the result of vertices data and perform a map operation.\n                    .getVertices()\n                    .map(v -> String.format("%s,%s", v.getId(), v.getValue()))\n                    .withParallelism(mapParallelism)\n                    .sink(sink)\n                    .withParallelism(sinkParallelism);\n            }\n        });\n\t\t\n        return pipeline.execute();\n    }\n\t\n\t// Implement Pagerank dynamic graph algorithm.\n    public static class IncGraphAlgorithms extends IncVertexCentricCompute<Integer, Integer,\n        Integer, Integer> {\n\n        public IncGraphAlgorithms(long iterations) {\n\t\t\t// Set the maximum number of iterations for the algorithm.\n            super(iterations);\n        }\n\n        @Override\n        public IncVertexCentricComputeFunction<Integer, Integer, Integer, Integer> getIncComputeFunction() {\n\t\t\t// Specify the Pagerank calculation logic.\n            return new PRVertexCentricComputeFunction();\n        }\n\n        @Override\n        public VertexCentricCombineFunction<Integer> getCombineFunction() {\n            return null;\n        }\n\n    }\n\n    public static class PRVertexCentricComputeFunction implements\n        IncVertexCentricComputeFunction<Integer, Integer, Integer, Integer> {\n\n        private IncGraphComputeContext<Integer, Integer, Integer, Integer> graphContext;\n\t\t\n\t\t// Init method, set graphContext.\n        @Override\n        public void init(IncGraphComputeContext<Integer, Integer, Integer, Integer> graphContext) {\t\n            this.graphContext = graphContext;\n        }\n\t\t\n\t\t// The first round of iteration implementation of the evolve method.\n        @Override\n        public void evolve(Integer vertexId,\n                           TemporaryGraph<Integer, Integer, Integer> temporaryGraph) {\n\t\t\t// Set the dynamic graph version to 0.\n            long lastVersionId = 0L;\n\t\t\t// Get the vertex whose ID is equal to vertexId from the incremental graph.\n            IVertex<Integer, Integer> vertex = temporaryGraph.getVertex();\n\t\t\t// Get the historical base graph.\n            HistoricalGraph<Integer, Integer, Integer> historicalGraph = graphContext\n                .getHistoricalGraph();\n            if (vertex == null) {\n\t\t\t\t// If there is no vertex with ID equal to vertexId in the incremental graph, get it from the historical graph.\n                vertex = historicalGraph.getSnapShot(lastVersionId).vertex().get();\n            }\n\n            if (vertex != null) {\n\t\t\t\t// Get all the outgoing edges corresponding to a vertex from the incremental graph.\n                List<IEdge<Integer, Integer>> newEs = temporaryGraph.getEdges();\n\t\t\t\t// Get all the outgoing edges corresponding to a vertex from the historical graph.\n                List<IEdge<Integer, Integer>> oldEs = historicalGraph.getSnapShot(lastVersionId)\n                    .edges().getOutEdges();\n                if (newEs != null) {\n                    for (IEdge<Integer, Integer> edge : newEs) {\n\t\t\t\t\t\t// Send a message with the content of vertexId to the targetId of all edges in the incremental graph.\n                        graphContext.sendMessage(edge.getTargetId(), vertexId);\n                    }\n                }\n                if (oldEs != null) {\n                    for (IEdge<Integer, Integer> edge : oldEs) {\n\t\t\t\t\t\t// Send a message with the content of vertexId to the targetId of all edges in the historical graph.\n                        graphContext.sendMessage(edge.getTargetId(), vertexId);\n                    }\n                }\n            }\n\n        }\n\n        @Override\n        public void compute(Integer vertexId, Iterator<Integer> messageIterator) {\n            int max = 0;\n\t\t\t// Iterate through all messages received by vertexId and take the maximum value.\n            while (messageIterator.hasNext()) {\n                int value = messageIterator.next();\n                max = max > value ? max : value;\n            }\n\t\t\t// Get the vertex whose ID is equal to vertexId from the incremental graph.\n            IVertex<Integer, Integer> vertex = graphContext.getTemporaryGraph().getVertex();\t\t\n\t\t\t// Get the vertex whose ID is equal to vertexId from the historical graph.\n            IVertex<Integer, Integer> historyVertex = graphContext.getHistoricalGraph().getSnapShot(0).vertex().get();\n\t\t\t// Take the maximum value between the attribute value of a vertex in the incremental graph and the maximum value of the messages.\n            if (vertex != null && max < vertex.getValue()) {\n                max = vertex.getValue();\n            }\n\t\t\t// Take the maximum value between the attribute value of a vertex in the historical graph and the maximum value of the messages.\n            if (historyVertex != null && max < historyVertex.getValue()) {\n                max = historyVertex.getValue();\n            }\n\t\t\t// Update the attribute value of a vertex in the incremental graph.\n            graphContext.getTemporaryGraph().updateVertexValue(max);\n        }\n\t\t\n\t\t\n        @Override\n        public void finish(Integer vertexId, MutableGraph<Integer, Integer, Integer> mutableGraph) {\n\t\t\t// Get the vertices and edges related to vertexId from the incremental graph.\n            IVertex<Integer, Integer> vertex = graphContext.getTemporaryGraph().getVertex();\n            List<IEdge<Integer, Integer>> edges = graphContext.getTemporaryGraph().getEdges();\n            if (vertex != null) {\n\t\t\t\t// Add the vertices in the incremental graph to the graph data.\n                mutableGraph.addVertex(0, vertex);\n                graphContext.collect(vertex);\n            } else {\n                LOGGER.info("not found vertex {} in temporaryGraph ", vertexId);\n            }\n            if (edges != null) {\n\t\t\t\t// Add the edges in the incremental graph to the graph data.\n                edges.stream().forEach(edge -> {\n                    mutableGraph.addEdge(0, edge);\n                });\n            }\n        }\n    }\n\n}\n\n'})}),"\n",(0,i.jsx)(t.h3,{id:"introduction-to-pagerank-static-graph-computing-example",children:"Introduction to PageRank Static Graph Computing Example"}),"\n",(0,i.jsx)(t.h4,{id:"example-code-1",children:"Example Code"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-java",children:'\npublic class PageRank {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(PageRank.class);\n\t\n\t// Computed result path.\n    public static final String RESULT_FILE_PATH = "./target/tmp/data/result/pagerank";\n\t\n\t// Comparison result path.\n    public static final String REF_FILE_PATH = "data/reference/pagerank";\n\n    public static void main(String[] args) {\n\t\t// Get execution environment.\n        Environment environment = EnvironmentFactory.onLocalEnvironment();\n\t\t// Perform job submission.\n        IPipelineResult<?> result = submit(environment);\n\t\t// Wait for execution to complete.\n        result.get();\n\t\t// Close the execution environment.\n        environment.shutdown();\n    }\n\n    public static IPipelineResult<?> submit(Environment environment) {\n\t\t// Build the task execution flow.\n        Pipeline pipeline = PipelineFactory.buildPipeline(environment);\n\t\t// Get the job environment configuration.\n        Configuration envConfig = environment.getEnvironmentContext().getConfig();\n\t\t// Specifies the path to save the calculation results.\n        envConfig.put(FileSink.OUTPUT_DIR, RESULT_FILE_PATH);\n\t\t\n\t\t// Submit the task and execute.\n        pipeline.submit((PipelineTask) pipelineTaskCxt -> {\n            Configuration conf = pipelineTaskCxt.getConfig();\n\t\t\t// 1. Build vertex data input source.\n            PWindowSource<IVertex<Integer, Double>> prVertices =\n                pipelineTaskCxt.buildSource(new FileSource<>("data/input/email_vertex",\n\t\t\t\t\t// Specifies the parsing format for each row of data.\n                    line -> {\n                        String[] fields = line.split(",");\n                        IVertex<Integer, Double> vertex = new ValueVertex<>(\n                            Integer.valueOf(fields[0]), Double.valueOf(fields[1]));\n                        return Collections.singletonList(vertex);\n                    }), AllWindow.getInstance())\n\t\t\t\t\t// Specifies the parallelism of vertex data sources.\n                    .withParallelism(conf.getInteger(ExampleConfigKeys.SOURCE_PARALLELISM));\n\t\t\t// 2. Build the edge data input source.\n            PWindowSource<IEdge<Integer, Integer>> prEdges = pipelineTaskCxt.buildSource(new FileSource<>("data/input/email_edge",\n\t\t\t\t// Specifies the parsing format for each row of data.\n                line -> {\n                    String[] fields = line.split(",");\n                    IEdge<Integer, Integer> edge = new ValueEdge<>(Integer.valueOf(fields[0]), Integer.valueOf(fields[1]), 1);\n                    return Collections.singletonList(edge);\n                }), AllWindow.getInstance())\n\t\t\t\t// Specifies the parallelism of edge data sources.\n                .withParallelism(conf.getInteger(ExampleConfigKeys.SOURCE_PARALLELISM));\n\t\t\t\n\t\t\t// The parallelism of iteration computation.\n            int iterationParallelism = conf.getInteger(ExampleConfigKeys.ITERATOR_PARALLELISM);\t\t\n\t\t\t// Define graphview.\n            GraphViewDesc graphViewDesc = GraphViewBuilder\n                .createGraphView(GraphViewBuilder.DEFAULT_GRAPH)\n\t\t\t\t// Specify the number of shards as 2.\n                .withShardNum(2)\n\t\t\t\t// Specify the backend type as memory.\n                .withBackend(BackendType.Memory)\n                .build();\n\t\t\t\n\t\t\t// Build a static graph based on vertex/edge data and the defined graph view.\n            PGraphWindow<Integer, Double, Integer> graphWindow =\n                pipelineTaskCxt.buildWindowStreamGraph(prVertices, prEdges, graphViewDesc);\n\t\t\t// Create sink method.\n            SinkFunction<IVertex<Integer, Double>> sink = ExampleSinkFunctionFactory.getSinkFunction(conf);\n\t\t\t// Specify the computation concurrency and execute the static computation method.\n            graphWindow.compute(new PRAlgorithms(3))\n                .compute(iterationParallelism)\n\t\t\t\t// Get the computed vertices result and output them according to the defined sink function.\n                .getVertices()\n                .sink(sink)\n                .withParallelism(conf.getInteger(ExampleConfigKeys.SINK_PARALLELISM));\n        });\n\n        return pipeline.execute();\n    }\n\n    public static class PRAlgorithms extends VertexCentricCompute<Integer, Double, Integer, Double> {\n\n        public PRAlgorithms(long iterations) {\n\t\t\t// Specify the iteration number for static graph computation.\n            super(iterations);\n        }\n\n        @Override\n        public VertexCentricComputeFunction<Integer, Double, Integer, Double> getComputeFunction() {\n            return new PRVertexCentricComputeFunction();\n        }\n\n        @Override\n        public VertexCentricCombineFunction<Double> getCombineFunction() {\n            return null;\n        }\n\n    }\n\n    public static class PRVertexCentricComputeFunction extends AbstractVcFunc<Integer, Double, Integer, Double> {\n\t\t// The implementation of the static graph computation method.\n        @Override\n        public void compute(Integer vertexId,\n                            Iterator<Double> messageIterator) {\n\t\t\t// Get the vertex from the static graph where the vertex ID equals vertexId.\n            IVertex<Integer, Double> vertex = this.context.vertex().get();\n            if (this.context.getIterationId() == 1) {\n\t\t\t\t// In the first iteration, send messages to neighboring nodes, and the message content is the attribute value of the vertex with vertexId.\n                this.context.sendMessageToNeighbors(vertex.getValue());\n            } else {\n                double sum = 0;\n                while (messageIterator.hasNext()) {\n                    double value = messageIterator.next();\n                    sum += value;\n                }\n\t\t\t\t// Sum up the received messages and set it as the attribute value of the current vertex.\n                this.context.setNewVertexValue(sum);\n            }\n        }\n\n    }\n}\n\n'})}),"\n",(0,i.jsx)(t.h3,{id:"introduction-to-wordcount-batch-computation-example",children:"Introduction to WordCount Batch Computation Example"}),"\n",(0,i.jsx)(t.h4,{id:"example-code-2",children:"Example Code"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-java",children:'\npublic class WordCountStream {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(WordCountStream.class);\n\t\n\t// Computed result path.\n    public static final String RESULT_FILE_PATH = "./target/tmp/data/result/wordcount";\n\t\n\t// Comparison result path.\n    public static final String REF_FILE_PATH = "data/reference/wordcount";\n\n    public static void main(String[] args) {\n\t\t// Get execution environment.\n        Environment environment = EnvironmentUtil.loadEnvironment(args);\n\t\t// Perform job submission.\n        IPipelineResult<?> result = submit(environment);\n        result.get();\n\t\t// Close the execution environment.\n        environment.shutdown();\n    }\n\n    public static IPipelineResult<?> submit(Environment environment) {\n\t\t// Build the task execution flow.\n        Pipeline pipeline = PipelineFactory.buildPipeline(environment);\n\t\t// Get the job environment configuration.\n        Configuration envConfig = environment.getEnvironmentContext().getConfig();\n\t\t// Turn off the materialize switch for dynamic streaming.\n        envConfig.getConfigMap().put(FrameworkConfigKeys.INC_STREAM_MATERIALIZE_DISABLE.getKey(), Boolean.TRUE.toString());\n\t\t// Specifies the path to save the calculation results.\n        envConfig.getConfigMap().put(FileSink.OUTPUT_DIR, RESULT_FILE_PATH);\n\n        pipeline.submit(new PipelineTask() {\n            @Override\n            public void execute(IPipelineTaskContext pipelineTaskCxt) {\n                Configuration conf = pipelineTaskCxt.getConfig();\n\n\t\t\t\t// Gets the input data stream.\n                PWindowSource<String> streamSource = pipelineTaskCxt.buildSource(\n                    new FileSource<String>("data/input/email_edge",\n\t\t\t\t\t\t// Specifies the parsing format for each row of data.\n                        line -> {\n                            String[] fields = line.split(",");\n                            return Collections.singletonList(fields[0]);\n\t\t\t\t\t\t\t\t// Define the size of the data window.\n                        }) {}, SizeTumblingWindow.of(5000))\n\t\t\t\t\t// Specifies the parallelism of input data sources.\n                    .withParallelism(conf.getInteger(ExampleConfigKeys.SOURCE_PARALLELISM));\n\n                SinkFunction<String> sink = ExampleSinkFunctionFactory.getSinkFunction(conf);\t\t\t\n                streamSource\n\t\t\t\t\t// Perform map operation on the data in the stream.\n                    .map(e -> Tuple.of(e, 1))\n\t\t\t\t\t// Key by.\n                    .keyBy(new KeySelectorFunc())\n\t\t\t\t\t// Reduce: the aggregation to count the number of identical data.\n                    .reduce(new CountFunc())\n\t\t\t\t\t// Specify the concurrency num of the operator.\n                    .withParallelism(conf.getInteger(ExampleConfigKeys.REDUCE_PARALLELISM))\n                    .map(v -> String.format("(%s,%s)", ((Tuple) v).f0, ((Tuple) v).f1))\n                    .sink(sink)\n                    .withParallelism(conf.getInteger(ExampleConfigKeys.SINK_PARALLELISM));\n            }\n        });\n\n        return pipeline.execute();\n    }\n\n    public static void validateResult() throws IOException {\n        ResultValidator.validateResult(REF_FILE_PATH, RESULT_FILE_PATH);\n    }\n\n\n    public static class MapFunc implements MapFunction<String, Tuple<String, Integer>> {\n\t\t// Implement the map method to convert each input word to a Tuple.\n        @Override\n        public Tuple<String, Integer> map(String value) {\n            LOGGER.info("MapFunc process value: {}", value);\n            return Tuple.of(value, 1);\n        }\n    }\n\n    public static class KeySelectorFunc implements KeySelector<Tuple<String, Integer>, Object> {\n\n        @Override\n        public Object getKey(Tuple<String, Integer> value) {\n            return value.f0;\n        }\n    }\n\n    public static class CountFunc implements ReduceFunction<Tuple<String, Integer>> {\n\n        @Override\n        public Tuple<String, Integer> reduce(Tuple<String, Integer> oldValue, Tuple<String, Integer> newValue) {\n            return Tuple.of(oldValue.f0, oldValue.f1 + newValue.f1);\n        }\n    }\n}\n\n'})}),"\n",(0,i.jsx)(t.h3,{id:"introduction-to-keyagg-stream-computation-example",children:"Introduction to KeyAgg stream computation example"}),"\n",(0,i.jsx)(t.h4,{id:"example-code-3",children:"Example Code"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-java",children:'\npublic class WindowStreamKeyAgg implements Serializable {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(WindowStreamKeyAgg.class);\n\t// Computed result path.\n    public static final String RESULT_FILE_PATH = "./target/tmp/data/result/wordcount";\n\t\n\t// Comparison result path.\n    public static final String REF_FILE_PATH = "data/reference/wordcount";\n\n    public static void main(String[] args) {\n\t\t// Get execution environment.\n        Environment environment = EnvironmentUtil.loadEnvironment(args);\n\t\t// Perform job submission.\n        IPipelineResult<?> result = submit(environment);\n        result.get();\n\t\t// Close the execution environment.\n        environment.shutdown();\n    }\n\n    public static IPipelineResult<?> submit(Environment environment) {\n\t\t// Build the task execution flow.\n        Pipeline pipeline = PipelineFactory.buildPipeline(environment);\n\t\t// Get the job environment configuration.\n        Configuration envConfig = environment.getEnvironmentContext().getConfig();\n\t\t// Specifies the path to save the calculation results.\n        envConfig.getConfigMap().put(FileSink.OUTPUT_DIR, RESULT_FILE_PATH);\n        pipeline.submit(new PipelineTask() {\n            @Override\n            public void execute(IPipelineTaskContext pipelineTaskCxt) {\n                Configuration conf = pipelineTaskCxt.getConfig();\n\n\t\t\t\t// Define the window size as 5000 and construct an input data stream.\n                PWindowSource<String> streamSource =\n                    pipelineTaskCxt.buildSource(new FileSource<String>("data/input"\n                    + "/email_edge", Collections::singletonList) {}, SizeTumblingWindow.of(5000));\n\n                SinkFunction<String> sink = ExampleSinkFunctionFactory.getSinkFunction(conf);\n                streamSource\n                    .flatMap(new FlatMapFunction<String, Long>() {\n                        @Override\n                        public void flatMap(String value, Collector collector) {\n\t\t\t\t\t\t\t// Flatmap implementation.\n                            String[] records = value.split(SPLIT);\n                            for (String record : records) {\n\t\t\t\t\t\t\t\t// Split and partition each line of data.\n                                collector.partition(Long.valueOf(record));\n                            }\n                        }\n                    })\n\t\t\t\t\t// Map.\n                    .map(p -> Tuple.of(p, p))\n\t\t\t\t\t// Key by.\n                    .keyBy(p -> ((long) ((Tuple) p).f0) % 7)\n                    .aggregate(new AggFunc())\n                    .withParallelism(conf.getInteger(AGG_PARALLELISM))\n                    .map(v -> String.format("%s,%s", ((Tuple) v).f0, ((Tuple) v).f1))\n                    .sink(sink).withParallelism(conf.getInteger(SINK_PARALLELISM));\n            }\n        });\n\n        return pipeline.execute();\n    }\n\n\t\n    public static class AggFunc implements\n        AggregateFunction<Tuple<Long, Long>, Tuple<Long, Long>, Tuple<Long, Long>> {\n\t\t\n\t\t// Define the accumulator implementation.\n        @Override\n        public Tuple<Long, Long> createAccumulator() {\n            return Tuple.of(0L, 0L);\n        }\n\t\t\n        @Override\n        public void add(Tuple<Long, Long> value, Tuple<Long, Long> accumulator) {\n\t\t\t// Add up the f1 values of two values with the same key.\n            accumulator.setF0(value.f0);\n            accumulator.setF1(value.f1 + accumulator.f1);\n        }\n\n        @Override\n        public Tuple<Long, Long> getResult(Tuple<Long, Long> accumulator) {\n\t\t\t// Return the result after accumulation.\n            return Tuple.of(accumulator.f0, accumulator.f1);\n        }\n\n        @Override\n        public Tuple<Long, Long> merge(Tuple<Long, Long> a, Tuple<Long, Long> b) {\n            return null;\n        }\n    }\n\n}\n\n\n'})})]})}function p(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>o,x:()=>l});var i=n(6540);const r={},a=i.createContext(r);function o(e){const t=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function l(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(a.Provider,{value:t},e.children)}}}]);