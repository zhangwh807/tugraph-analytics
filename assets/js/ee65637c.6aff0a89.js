"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[1789],{1083:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/llm_ggml_model-fdd30b6f6640c4def399eef562ac03ba.png"},2710:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/llm_quantization_table-adc2d8cd29b0e371a75ccf6e3874053d.png"},3271:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/llm_hugging_face-cbdf155bf78c91051ef4b92c614e786f.png"},5503:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>a,toc:()=>d});var o=t(4848),l=t(8453);const i={},r="LLM Local Deployment",a={id:"deploy/install_llm",title:"LLM Local Deployment",description:"The users have the capability to locally deploy extensive models as a service. The complete process, encompassing downloading pre-trained models, deploying them as a service, and debugging, is described in the following steps. It is essential for the user's machine to have Docker installed and be granted access to the repository containing these large models.",source:"@site/../docs-en/source/7.deploy/5.install_llm.md",sourceDirName:"7.deploy",slug:"/deploy/install_llm",permalink:"/tugraph-analytics/en/deploy/install_llm",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:5,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"\ud83c\udf08 G6VP Graph Visualization",permalink:"/tugraph-analytics/en/deploy/collaborate_with_g6vp"},next:{title:"Install Minikube",permalink:"/tugraph-analytics/en/deploy/install_minikube"}},s={},d=[{value:"Step 1: Download the Model File",id:"step-1-download-the-model-file",level:2},{value:"Step 2: Prepare the Docker Container Environment",id:"step-2-prepare-the-docker-container-environment",level:2},{value:"Step 3: Model Service Deployment",id:"step-3-model-service-deployment",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",...(0,l.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"llm-local-deployment",children:"LLM Local Deployment"})}),"\n",(0,o.jsx)(n.p,{children:"The users have the capability to locally deploy extensive models as a service. The complete process, encompassing downloading pre-trained models, deploying them as a service, and debugging, is described in the following steps. It is essential for the user's machine to have Docker installed and be granted access to the repository containing these large models."}),"\n",(0,o.jsx)(n.h2,{id:"step-1-download-the-model-file",children:"Step 1: Download the Model File"}),"\n",(0,o.jsxs)(n.p,{children:["The pre-trained large model file has been uploaded to the ",(0,o.jsx)(n.a,{href:"https://huggingface.co/tugraph/CodeLlama-7b-GQL-hf",children:"Hugging Face repository"}),". Please proceed with downloading and locally unzipping the model file.\n",(0,o.jsx)(n.img,{alt:"hugging",src:t(3271).A+"",width:"3122",height:"1776"})]}),"\n",(0,o.jsx)(n.h2,{id:"step-2-prepare-the-docker-container-environment",children:"Step 2: Prepare the Docker Container Environment"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Run the following command on the terminal to download the Docker image required for model servicing:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"docker pull tugraph/llam_infer_service:0.0.1\n\n// Use the following command to verify that the image was successfully downloaded\n\ndocker images\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Run the following command to start the Docker container:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"docker run -it  --name ${Container name} -v ${Local model path}:${Container model path} -p ${Local port}:${Container service port} -d ${Image name}\n\n// Such as\ndocker run -it --name my-model-container -v /home/huggingface:/opt/huggingface -p 8000:8000 -d llama_inference_server:v1\n\n// Check whether the container is running properly\ndocker ps\n"})}),"\n",(0,o.jsx)(n.p,{children:"Here, we map the container's port 8000 to the local machine's port 8000, mount the directory where the local model (/home/huggingface) resides to the container's path (/opt/huggingface), and set the container name to my-model-container."}),"\n",(0,o.jsx)(n.h2,{id:"step-3-model-service-deployment",children:"Step 3: Model Service Deployment"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Model transformation"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"// Enter the container you just created\ndocker exec -it ${container_id} bash\n\n// Execute the following command\ncd /opt/llama_cpp\npython3 ./convert.py ${Container model path}\n"})}),"\n",(0,o.jsxs)(n.p,{children:["When the execution is complete, a file with the prefix ggml-model is generated under the container model path.\n",(0,o.jsx)(n.img,{alt:"undefined",src:t(1083).A+"",width:"1994",height:"184"})]}),"\n",(0,o.jsxs)(n.ol,{start:"2",children:["\n",(0,o.jsx)(n.li,{children:"Model quantization (optional)\nTake the llam2-7B model as an example: By default, the accuracy of the model converted by convert.py is F16 and the model size is 13.0GB. If the current machine resources cannot satisfy such a large model inference, the converted model can be further quantized by./quantize."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"// As shown below, q4_0 quantizes the original model to int4 and compresses the model size to 3.5GB\n\ncd /opt/llama_cpp\n./quantize ${Default generated F16 model path} ${Quantized model path} q4_0\n"})}),"\n",(0,o.jsxs)(n.p,{children:["The following are reference indicators such as the size and reasoning speed of the quantized model:\n",(0,o.jsx)(n.img,{alt:"undefined",src:t(2710).A+"",width:"1374",height:"850"})]}),"\n",(0,o.jsxs)(n.ol,{start:"3",children:["\n",(0,o.jsx)(n.li,{children:"Model servicing\nRun the following command to deploy the above generated model as a service, and specify the address and port of the service binding through the parameters:"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"// ./server -h. You can view parameter details\n// ${ggml-model...file} The file name prefixes the generated ggml-model\n\ncd /opt/llama_cpp\n./server --host ${ip} --port ${port} -m ${Container model path}/${ggml-model...file} -c 4096\n\n// Such as\n./server --host 0.0.0.0 --port 8000 -m  /opt/huggingface/ggml-model-f16.gguf -c 4096\n"})}),"\n",(0,o.jsxs)(n.ol,{start:"4",children:["\n",(0,o.jsx)(n.li,{children:'Debugging service\nSend an http request to the service address, where "prompt" is the query statement and "content" is the inference result.'}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:'curl --request POST \\\n    --url http://127.0.0.1:8000/completion \\\n    --header "Content-Type: application/json" \\\n    --data \'{"prompt": "\u8bf7\u8fd4\u56de\u5c0f\u7ea2\u768410\u4e2a\u5e74\u9f84\u5927\u4e8e20\u7684\u670b\u53cb","n_predict": 128}\'\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Debugging service\nThe following is the model inference result after service deployment:\n",(0,o.jsx)(n.img,{alt:"undefined",src:t(9336).A+"",width:"3452",height:"378"})]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var o=t(6540);const l={},i=o.createContext(l);function r(e){const n=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:r(e.components),o.createElement(i.Provider,{value:n},e.children)}},9336:(e,n,t)=>{t.d(n,{A:()=>o});const o=t.p+"assets/images/llm_chat_result-be8e98ff482447100ea379b3f32f625f.png"}}]);